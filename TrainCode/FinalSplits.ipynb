{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PART OF PREPARING THE DATA"
      ],
      "metadata": {
        "id": "-F0rV5CFLajZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSTvv3n67h5E",
        "outputId": "842ff393-99ca-45be-eebe-5dba1236168c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/TFG - Irony Detection - Èric Casanovas Pérez/Code/Tests Bert\n",
            "mkdir: cannot create directory ‘datasetFINAL’: File exists\n",
            "mkdir: cannot create directory ‘datasetFINAL/train_ds’: File exists\n",
            "mkdir: cannot create directory ‘datasetFINAL/test_ds’: File exists\n",
            "mkdir: cannot create directory ‘datasetFINAL/validation_ds’: File exists\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Go to the current working folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/TFG - Irony Detection - Èric Casanovas Pérez/Code/Tests Bert\n",
        "\n",
        "# Create the necessary folders\n",
        "!mkdir datasetFINAL\n",
        "!mkdir datasetFINAL/train_ds\n",
        "!mkdir datasetFINAL/test_ds\n",
        "!mkdir datasetFINAL/validation_ds\n",
        "\n",
        "# Get the initial dataset, do some cleaning\n",
        "df = pd.read_csv('ds.csv')\n",
        "df = df.drop(['ID'], axis=1)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Do the trin / test / validation splits\n",
        "dftrain, dfaux = train_test_split(df, test_size=0.2)\n",
        "dftest, dfvalitaion = train_test_split(dfaux, test_size=0.2)\n",
        "\n",
        "\n",
        "# Save the DataFrame as a new CSV file\n",
        "dftrain.to_csv('datasetFINAL/train_ds/data.csv', index=False)\n",
        "dftest.to_csv('datasetFINAL/train_ds/data.csv', index=False)\n",
        "dfvalitaion.to_csv('datasetFINAL/train_ds/data.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.to_csv('trainDataSet.csv', index=False)\n",
        "dftest.to_csv('testDataSet.csv', index=False)\n",
        "dfvalitaion.to_csv('validationDataSet.csv', index=False)"
      ],
      "metadata": {
        "id": "a6UxPB5wAwJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART OF PREPARING THE DATA FOR THE MODEL"
      ],
      "metadata": {
        "id": "qS-9lv0fLXmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Just in case, create the folders\n",
        "!mkdir datasetFINAL\n",
        "!mkdir datasetFINAL/train_ds\n",
        "!mkdir datasetFINAL/train_ds/ironic\n",
        "!mkdir datasetFINAL/train_ds/notironic\n",
        "!mkdir datasetFINAL/test_ds\n",
        "!mkdir datasetFINAL/test_ds/ironic\n",
        "!mkdir datasetFINAL/test_ds/notironic\n",
        "!mkdir datasetFINAL/validation_ds\n",
        "!mkdir datasetFINAL/validation_ds/ironic\n",
        "!mkdir datasetFINAL/validation_ds/notironic\n",
        "\n",
        "\n",
        "# Get the train dataset and do the correspondent splits\n",
        "train = pd.read_csv('datasetFINAL/train_ds/trainDataSet.csv')\n",
        "\n",
        "# Drop unecessary cols + reset the indx\n",
        "train = train.drop(['TOPIC'], axis=1)\n",
        "train = train.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# For each row of the dataset\n",
        "for ind in train.index:\n",
        "  if(train.iloc[ind]['IS_IRONIC'] == 0):\n",
        "    with open('datasetFINAL/train_ds/notironic/'+str(ind) +'.txt', 'w') as writefile:\n",
        "        writefile.write(train.iloc[ind]['MESSAGE'])\n",
        "  else:\n",
        "    with open('datasetFINAL/train_ds/ironic/'+str(ind) +'.txt', 'w') as writefile:\n",
        "        writefile.write(train.iloc[ind]['MESSAGE'])\n",
        "\n",
        "# Get the test dataset and do the correspondent splits\n",
        "test = pd.read_csv('datasetFINAL/test_ds/testDataSet.csv')\n",
        "\n",
        "# Drop unecessary cols + reset the indx\n",
        "test = test.drop(['TOPIC'], axis=1)\n",
        "test = test.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# For each row of the dataset\n",
        "for ind in test.index:\n",
        "  if(test.iloc[ind]['IS_IRONIC'] == 0):\n",
        "    with open('datasetFINAL/test_ds/notironic/'+str(ind) +'.txt', 'w') as writefile:\n",
        "        writefile.write(test.iloc[ind]['MESSAGE'])\n",
        "  else:\n",
        "    with open('datasetFINAL/test_ds/ironic/'+str(ind) +'.txt', 'w') as writefile:\n",
        "        writefile.write(test.iloc[ind]['MESSAGE'])\n",
        "\n",
        "# Get the validation dataset and do the correspondent splits\n",
        "validation = pd.read_csv('datasetFINAL/validation_ds/validationDataSet.csv')\n",
        "\n",
        "# Drop unecessary cols + reset the indx\n",
        "validation = validation.drop(['TOPIC'], axis=1)\n",
        "validation = validation.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# For each row of the dataset\n",
        "for ind in validation.index:\n",
        "  if(validation.iloc[ind]['IS_IRONIC'] == 0):\n",
        "    with open('datasetFINAL/validation_ds/notironic/'+str(ind) +'.txt', 'w') as writefile:\n",
        "        writefile.write(validation.iloc[ind]['MESSAGE'])\n",
        "  else:\n",
        "    with open('datasetFINAL/validation_ds/ironic/'+str(ind) +'.txt', 'w') as writefile:\n",
        "        writefile.write(validation.iloc[ind]['MESSAGE'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJuLM5f2LXaH",
        "outputId": "7373aca0-52d6-4efa-8177-7c04fededa9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘datasetFINAL’: File exists\n",
            "mkdir: cannot create directory ‘datasetFINAL/train_ds’: File exists\n",
            "mkdir: cannot create directory ‘datasetFINAL/test_ds’: File exists\n",
            "mkdir: cannot create directory ‘datasetFINAL/validation_ds’: File exists\n"
          ]
        }
      ]
    }
  ]
}